
experiment:
  problem: RW
  instances:
    - RW400  # 4.0 meters free height
    - RW500  # 5.0 meters free height
    - RW600  # 6.0 meters free height

  metaheuristics:
    - GWO  # Grey Wolf Optimizer
    - PSO  # Particle Swarm Optimization
    - SCA  # Sine Cosine Algorithm
  

  machine_learning:
    - QL   # Q-Learning
    - BQSA # Backward Q-Learning with SARSA
    - BCL  # Basic (no ML)
  

  parameters:
    runs: 50
    # Population size (increased for larger instances)
    population: 20
    
    max_iterations: 1000
    # Options: 40a, 80a, ver1-ver90
    discretization_schemes:
      - 40a
      - 80a
    
    # Reward type indices (0-7)
    # 0: withPenalty1
    # 1: withoutPenalty1
    # 2: globalBest
    # 3: rootAdaptation
    # 4: escalatingMultiplicativeAdaptation
    # 5: percentageImprovement
    # 6: percentageImprovementAndDeterioration
    # 7: percentageImprovementAndDeteriorationWithIter
    reward_types: [5, 6]
    
    # Policy type indices (0-4)
    # 0: e-greedy
    # 1: greedy
    # 2: e-soft
    # 3: softMax-rulette
    # 4: softMax-rulette-elitist
    policy_types: [0, 4]
    
    # ML-specific parameters
    epsilon: 0.1
    states_q: 2
    W: 10
    visit_all_once: true
    ql_alpha: 0.1
    ql_gamma: 0.4
    ql_alpha_type: static  # static, iteration, or visits
    beta_dis: 0.8
    cond_backward: 10
    
    # MH-specific parameters
    a_SCA: 2
    b_WOA: 1
    beta_HHO: 1.5
    pa_CS: 0.25
    alpha_CS: 1
    beta_CS: 1.5
    Vmax_PSO: 6
    wMax_PSO: 0.9
    wMin_PSO: 0.2
    c1_PSO: 2
    c2_PSO: 2
  
  # Problem-specific parameters for RW
  problem_params:
    FO: min  # Optimization direction: minimize (cost, emissions, etc.)
    lb: -10  # Lower bound for continuous solutions (not used in RW)
    ub: 10   # Upper bound for continuous solutions (not used in RW)
    repair_type: 2  # Repair strategy type
    instance_dir: RW/  # Directory for RW instances (empty, RW doesn't need files)
